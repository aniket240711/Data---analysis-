{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**KNN & PCA | Assignment**"
      ],
      "metadata": {
        "id": "P158TUuycPZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?"
      ],
      "metadata": {
        "id": "q6eI5W3fcTjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - K-Nearest Neighbors (KNN) is a simple, non-parametric, supervised machine learning algorithm that can be used for both classification and regression tasks.\n",
        "\n",
        "- How KNN Works:\n",
        "\n",
        "1. For Classification:\n",
        "\n",
        "\n",
        "When given a new data point, KNN looks at its K nearest neighbors in the training data.\n",
        "\n",
        "It then assigns the new data point to the class that is most common among these K neighbors (a 'majority vote').\n",
        "\n",
        "\n",
        "2. For Regression:\n",
        "\n",
        "Similar to classification, it identifies the K nearest neighbors.\n",
        "\n",
        "\n",
        "Instead of a majority vote, it takes the average (or median) of the target values of these K neighbors as the prediction for the new data point.\n"
      ],
      "metadata": {
        "id": "nz3q_r2IcqOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?"
      ],
      "metadata": {
        "id": "-olUutpYc9Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The Curse of Dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (many features/variables) that do not occur in low-dimensional settings. As the number of dimensions increases, the volume of the space increases so rapidly that the available data becomes sparse, making it difficult to find meaningful patterns or relationships.\n",
        "\n",
        "- How the Curse of Dimensionality Affects KNN Performance:\n",
        "\n",
        "In high-dimensional spaces, the distance between any two data points tends to become more uniform, making it difficult for KNN to effectively distinguish between 'near' and 'far' neighbors.\n",
        "\n",
        " This can lead to:\n",
        "\n",
        "1-  Increased Computational Cost: Calculating distances in many dimensions is computationally expensive.\n",
        "\n",
        "2- Reduced Performance: The concept of 'nearest neighbor' becomes less meaningful, as all points can appear equidistant from each other. This can lead to KNN considering points that are not truly similar as neighbors, thus reducing its accuracy and predictive power.\n",
        "\n",
        "3- More Data Required: To maintain statistical significance, the amount of data needed grows exponentially with the number of dimensions.\n"
      ],
      "metadata": {
        "id": "_PMpewWDdDTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?"
      ],
      "metadata": {
        "id": "0wkPsjsKdoLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Principal Component Analysis (PCA) is a popular unsupervised dimensionality reduction technique. It transforms a dataset of possibly correlated variables into a set of linearly uncorrelated variables called principal components. The goal is to retain as much of the original variance in the data as possible with a reduced number of dimensions.\n",
        "\n",
        "- PCA vs. Feature Selection:\n",
        "\n",
        "1- Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new, smaller set of uncorrelated features (principal components) while retaining most of the variance. It creates new features.\n",
        "\n",
        "2- Feature Selection is a process that selects a subset of the original features based on their relevance or importance. It chooses from existing features, rather than creating new ones."
      ],
      "metadata": {
        "id": "DImUMGZVdsS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?"
      ],
      "metadata": {
        "id": "SKq78RmAfFHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental mathematical concepts:\n",
        "\n",
        "\n",
        "1- Eigenvectors: These represent the directions or principal components along which the data varies the most. They indicate the orientation of the new axes in the transformed space. Each eigenvector points in a direction that captures a different aspect of the data's variance. In PCA, the first eigenvector points in the direction of the highest variance, the second in the direction of the second highest variance (orthogonal to the first), and so on.\n",
        "\n",
        "2- Eigenvalues: These are scalar values corresponding to each eigenvector. An eigenvalue quantifies the amount of variance in the data along its corresponding eigenvector. A larger eigenvalue means that its eigenvector captures more variance from the dataset.\n",
        "\n",
        "\n",
        "Importance in PCA: Eigenvalues and eigenvectors are crucial because they allow PCA to:\n",
        "\n",
        "\n",
        "1- Determine Principal Components: The eigenvectors define the principal components, which are the new dimensions of the dataset.\n",
        "\n",
        "2- Order Components: Eigenvalues allow us to rank the principal components by their importance, with larger eigenvalues indicating more significant components that retain more information.\n",
        "\n",
        "3- Reduce Dimensionality: By selecting only the eigenvectors with the largest eigenvalues, we can reduce the dimensionality of the data while retaining the most significant variance, thus preserving the most important information.\n"
      ],
      "metadata": {
        "id": "ZRrUE0X7fHeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?"
      ],
      "metadata": {
        "id": "AMrYZphlfeLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) complement each other effectively in a single pipeline, primarily because PCA can mitigate some of KNN's inherent weaknesses, especially in high-dimensional datasets.\n",
        "\n",
        " Here’s how:\n",
        "\n",
        "- Addressing the Curse of Dimensionality: As discussed, KNN suffers from the curse of dimensionality, where distances become less meaningful in high-dimensional spaces, leading to reduced accuracy and increased sparsity of data. PCA, being a dimensionality reduction technique, can project the high-dimensional data into a lower-dimensional subspace while retaining most of the significant variance.\n",
        "\n",
        "- Improving Computational Efficiency: Calculating distances between data points is computationally expensive in high-dimensional spaces. By reducing the number of features with PCA, the distance calculations for KNN become much faster, leading to a more efficient algorithm.\n",
        "\n",
        "- Enhancing KNN Performance: When the irrelevant or noisy features are removed and the most discriminative information is compressed into fewer principal components by PCA, KNN can find more meaningful 'neighbors'. This often leads to improved accuracy and robustness of the KNN model.\n",
        "\n",
        "- Noise Reduction: PCA can also help in reducing noise in the data by discarding principal components that capture very little variance, which might mostly consist of noise.\n",
        "\n",
        "In essence, PCA acts as a pre-processing step for KNN, transforming the data into a more manageable and informative representation, thereby allowing KNN to perform better, faster, and more reliably.\n",
        "\n"
      ],
      "metadata": {
        "id": "fX4fbIjVfnXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "WH7d6Sftf4Dl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514f51cb",
        "outputId": "bd9a38eb-6f6d-4461-e04e-0291174e89cd"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (142, 13)\n",
            "X_test shape: (36, 13)\n",
            "y_train shape: (142,)\n",
            "y_test shape: (36,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beaeef77",
        "outputId": "2a255c37-e83e-4d76-e948-a764d41b6929"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train KNN without scaling\n",
        "knn_unscaled = KNeighborsClassifier()\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the unscaled test set\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"KNN Accuracy (without feature scaling): {accuracy_unscaled:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (without feature scaling): 0.7222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ea5eb1",
        "outputId": "aa4206a7-e73c-4917-e3de-4e001e04ca91"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features scaled using StandardScaler.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features scaled using StandardScaler.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd365a5",
        "outputId": "6b37ec98-503c-46b3-d754-20970b7a95e9"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train KNN with scaling\n",
        "knn_scaled = KNeighborsClassifier()\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test set\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"KNN Accuracy (with feature scaling): {accuracy_scaled:.4f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (with feature scaling): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7954afde",
        "outputId": "8d5d7ea8-ca85-4ed6-987e-9b42ee06e841"
      },
      "source": [
        "print(f\"\\n--- Comparison of KNN Accuracy ---\")\n",
        "print(f\"KNN Accuracy (without feature scaling): {accuracy_unscaled:.4f}\")\n",
        "print(f\"KNN Accuracy (with feature scaling): {accuracy_scaled:.4f}\")\n",
        "\n",
        "accuracy_difference = accuracy_scaled - accuracy_unscaled\n",
        "print(f\"\\nDifference in accuracy (Scaled - Unscaled): {accuracy_difference:.4f}\")\n",
        "\n",
        "if accuracy_difference > 0:\n",
        "    print(f\"Conclusion: Feature scaling significantly improved the KNN model's accuracy by {accuracy_difference:.4f}.\")\n",
        "elif accuracy_difference < 0:\n",
        "    print(f\"Conclusion: Feature scaling decreased the KNN model's accuracy by {abs(accuracy_difference):.4f}.\")\n",
        "else:\n",
        "    print(f\"Conclusion: Feature scaling had no significant impact on the KNN model's accuracy.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparison of KNN Accuracy ---\n",
            "KNN Accuracy (without feature scaling): 0.7222\n",
            "KNN Accuracy (with feature scaling): 0.9444\n",
            "\n",
            "Difference in accuracy (Scaled - Unscaled): 0.2222\n",
            "Conclusion: Feature scaling significantly improved the KNN model's accuracy by 0.2222.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of KNN Accuracies\n",
        "After training K-Nearest Neighbors (KNN) classifiers on the Wine dataset both with and without feature scaling, the following accuracies were observed:\n",
        "\n",
        "KNN Accuracy (without feature scaling): 0.7222\n",
        "KNN Accuracy (with feature scaling): 0.9444\n",
        "Summary:\n",
        "\n",
        "Feature scaling, specifically using StandardScaler, significantly improved the performance of the KNN classifier on the Wine dataset. Without scaling, the model achieved an accuracy of approximately 72.22%.\n",
        "\n",
        "However, after applying feature scaling, the accuracy rose to approximately 94.44%. This substantial increase demonstrates the critical importance of feature scaling for distance-based algorithms like KNN, as it ensures that all features contribute equally to the distance calculations, preventing features with larger ranges from dominating the distance metric"
      ],
      "metadata": {
        "id": "9ffWlTwtg9Yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "pOmvpIiihCXG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cac96e8",
        "outputId": "e32255a4-da90-4751-c06b-2b213aca2d8a"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA. We'll start without specifying n_components to see the variance explained by all components.\n",
        "pca = PCA()\n",
        "\n",
        "# Fit PCA on the scaled training data\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Print the explained variance ratio of each principal component\n",
        "print('Explained variance ratio of each principal component:')\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f'Principal Component {i+1}: {ratio:.4f}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "Principal Component 1: 0.3590\n",
            "Principal Component 2: 0.1869\n",
            "Principal Component 3: 0.1161\n",
            "Principal Component 4: 0.0737\n",
            "Principal Component 5: 0.0665\n",
            "Principal Component 6: 0.0485\n",
            "Principal Component 7: 0.0420\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0235\n",
            "Principal Component 10: 0.0189\n",
            "Principal Component 11: 0.0172\n",
            "Principal Component 12: 0.0126\n",
            "Principal Component 13: 0.0083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a816a7d"
      },
      "source": [
        "The explained variance ratio indicates the proportion of the dataset's variance that lies along each principal component. A higher ratio for a component means it captures more information (variance) from the original features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "GtpUzsW_ha_e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4a9f1c0"
      },
      "source": [
        "# Task\n",
        "Train a K-Nearest Neighbors (KNN) classifier on the Wine dataset after applying Principal Component Analysis (PCA) to reduce the data to its top 2 components. Then, compare the accuracy of this PCA-transformed KNN model with the accuracies of KNN models trained on the unscaled and scaled original datasets (from Question 6), and summarize the impact of dimensionality reduction via PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24eac082"
      },
      "source": [
        "## Apply PCA with 2 Components\n",
        "\n",
        "### Subtask:\n",
        "Apply Principal Component Analysis (PCA) to the scaled training and testing data (`X_train_scaled`, `X_test_scaled`), retaining only the top 2 principal components. This will reduce the dimensionality of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1a62394"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires applying PCA with 2 components to the scaled training and testing data. This step will initialize PCA, fit it on the training data, and then transform both training and testing sets, storing the results in new variables as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a9b7828",
        "outputId": "cf7d9190-3198-48c1-d0c2-ef525de61a45"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Initialize PCA with n_components=2\n",
        "pca_2_components = PCA(n_components=2)\n",
        "\n",
        "# 2. Fit PCA on the scaled training data\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "\n",
        "# 3. Transform both training and testing data\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Original training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"PCA transformed training data shape: {X_train_pca.shape}\")\n",
        "print(f\"Original testing data shape: {X_test_scaled.shape}\")\n",
        "print(f\"PCA transformed testing data shape: {X_test_pca.shape}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training data shape: (142, 13)\n",
            "PCA transformed training data shape: (142, 2)\n",
            "Original testing data shape: (36, 13)\n",
            "PCA transformed testing data shape: (36, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c40530e8",
        "outputId": "2a96ca9a-a52c-4f08-e231-73a1e41981b3"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize KNN classifier\n",
        "knn_pca = KNeighborsClassifier()\n",
        "\n",
        "# Fit KNN on PCA-transformed training data\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on PCA-transformed test data\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"KNN Accuracy (on PCA-transformed data with 2 components): {accuracy_pca:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (on PCA-transformed data with 2 components): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02f53715",
        "outputId": "56108d69-7689-4b70-8f1f-f159daa647df"
      },
      "source": [
        "print(f\"\\n--- Comparison of KNN Accuracy ---\")\n",
        "print(f\"KNN Accuracy (without feature scaling): {accuracy_unscaled:.4f}\")\n",
        "print(f\"KNN Accuracy (with feature scaling): {accuracy_scaled:.4f}\")\n",
        "print(f\"KNN Accuracy (on PCA-transformed data with 2 components): {accuracy_pca:.4f}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparison of KNN Accuracy ---\n",
            "KNN Accuracy (without feature scaling): 0.7222\n",
            "KNN Accuracy (with feature scaling): 0.9444\n",
            "KNN Accuracy (on PCA-transformed data with 2 components): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this specific Wine dataset, the top 2 principal components evidently capture almost all the necessary information to perfectly distinguish between the wine classes, leading to a perfect accuracy score for the KNN model. This highlights PCA's power as a pre-processing step for algorithms like KNN, especially when dealing with datasets that might contain correlated features or noise."
      ],
      "metadata": {
        "id": "G0_8c8KRi9Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "cxr52MExi-O9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d45ad4a",
        "outputId": "c676f2ce-1219-4ca0-c055-5eab228709ee"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize KNN classifier with Euclidean distance metric\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "\n",
        "# Fit KNN on scaled training data\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on scaled test data\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "print(f\"KNN Accuracy (Euclidean distance on scaled data): {accuracy_euclidean:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (Euclidean distance on scaled data): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4584a028",
        "outputId": "c8a328c1-8aa7-4832-96d4-de0f04583cd8"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize KNN classifier with Manhattan distance metric\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "# Fit KNN on scaled training data\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on scaled test data\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(f\"KNN Accuracy (Manhattan distance on scaled data): {accuracy_manhattan:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (Manhattan distance on scaled data): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5201407f",
        "outputId": "5aeb675e-bb3d-4375-db98-37839d7c73d8"
      },
      "source": [
        "print(f\"KNN Accuracy (Euclidean distance on scaled data): {accuracy_euclidean:.4f}\")\n",
        "print(f\"KNN Accuracy (Manhattan distance on scaled data): {accuracy_manhattan:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy (Euclidean distance on scaled data): 0.9444\n",
            "KNN Accuracy (Manhattan distance on scaled data): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "rXPrHvWskhY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Use PCA to reduce dimensionality:\n",
        "\n",
        "\n",
        "Data Preprocessing: First, I would ensure the gene expression data is properly preprocessed. This typically involves normalization (e.g., Z-score scaling or StandardScaler) to bring all genes to a similar scale, as PCA is sensitive to the variance of features. Missing values would also need to be handled appropriately.\n",
        "\n",
        "Applying PCA: I would then apply PCA to the scaled gene expression data. PCA will transform the original (likely correlated) gene expression features into a new set of orthogonal (uncorrelated) variables called Principal Components (PCs). These PCs capture the maximum variance in the data, with the first few PCs representing the most significant patterns.\n",
        "\n",
        "2. Decide how many components to keep:\n",
        "\n",
        "\n",
        "Scree Plot: I would generate a scree plot, which displays the eigenvalues (variance explained) for each principal component in descending order. I'd look for an 'elbow' point where the marginal gain in explained variance drops significantly, indicating that subsequent components contribute less information.\n",
        "\n",
        "Cumulative Explained Variance: I would also examine the cumulative explained variance plot. This plot shows the total proportion of variance explained as more principal components are added. A common practice is to select a number of components that explain a high percentage of the total variance, such as 90% or 95%, while significantly reducing the number of features. Given the small sample size, aiming for a smaller number of components to avoid overfitting is critical.\n",
        "\n",
        "Cross-Validation: For more robustness, I could also use cross-validation to test the performance of the downstream KNN classifier with different numbers of principal components and select the number that yields the best generalization performance.\n",
        "\n",
        "3. Use KNN for classification post-dimensionality reduction:\n",
        "\n",
        "\n",
        "Transformed Data: Once the optimal number of principal components is determined, the original gene expression data would be transformed into this lower-dimensional PCA space. All subsequent steps, including training and testing the KNN model, would use this PCA-transformed data.\n",
        "\n",
        "KNN Application: A K-Nearest Neighbors (KNN) classifier would then be trained on this reduced-dimensional data. For a new patient's gene expression profile, it would be projected into the same PCA space, and its class would be determined by the majority class among its K nearest neighbors in that space.\n",
        "\n",
        "4. Evaluate the model:\n",
        "\n",
        "\n",
        "Train-Test Split/Cross-Validation: The dataset would be split into training and testing sets (or utilize k-fold cross-validation, especially given the small sample size) before PCA is applied to prevent data leakage. PCA parameters (fitting) would be learned only on the training set.\n",
        "\n",
        "Metrics: I would evaluate the model using appropriate metrics for classification, considering the potential for imbalanced classes in cancer datasets:\n",
        "\n",
        "- Accuracy: Overall correct predictions.\n",
        "Precision, Recall, F1-score: To assess performance for each cancer type, especially for minority classes.\n",
        "\n",
        "- ROC AUC: For binary or multi-class classification, indicating the model's ability to distinguish between classes.\n",
        "\n",
        "- Confusion Matrix: To visualize the types of correct and incorrect predictions.\n",
        "\n",
        "Overfitting Check: I'd compare training and test set performance. If training performance is significantly better, it indicates overfitting, and further adjustments to K in KNN or the number of PCA components might be needed.\n",
        "\n",
        "5. Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data:\n",
        "\n",
        "\n",
        "- Mitigating Overfitting: This pipeline directly addresses the problem of overfitting common in high-dimensional, small-sample biomedical datasets. PCA reduces the feature space, transforming many correlated genes into a few uncorrelated principal components, which makes the learning task simpler and reduces the chances of the model memorizing noise in the training data.\n",
        "\n",
        "- Handling the Curse of Dimensionality: Biomedical data often suffers from the 'curse of dimensionality,' where distances become less meaningful. PCA effectively projects data into a lower-dimensional space where distance calculations for KNN are more reliable and meaningful, leading to better generalization.\n",
        "\n",
        "-  Interpretability (to some extent): While PCs themselves are abstract, the approach can highlight the most significant patterns in gene expression without requiring domain experts to select individual genes (which can be biased or miss complex interactions). We can analyze the loadings of the original genes on the top PCs to gain some biological insights.\n",
        "\n",
        "- Computational Efficiency: Reducing dimensionality makes the KNN algorithm, which is distance-intensive, computationally faster, allowing for quicker analysis and predictions.\n",
        "\n",
        "- Non-Parametric Nature of KNN: KNN makes no assumptions about the underlying data distribution, which can be beneficial for complex biomedical data where parametric assumptions might not hold.\n",
        "\n",
        "- Proven Methodology: Both PCA and KNN are well-established and widely used techniques in machine learning and bioinformatics, providing a trustworthy and understandable framework for analysis. The combination offers a robust and effective approach to extracting meaningful signals from complex gene expression data, which is crucial for reliable cancer classification in a real-world clinical context.\n"
      ],
      "metadata": {
        "id": "tSmgmEiDk5yc"
      }
    }
  ]
}